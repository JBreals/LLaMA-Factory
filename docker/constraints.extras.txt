# Quantization + monitoring extras (CUDA 12.x, torch ~2.6)
bitsandbytes==0.43.3
gptqmodel==2.0.0
# awq 0.2.x line; pin to a stable minor
awq==0.2.8
aqlm[gpu]>=1.1.0
hqq
eetq
swanlab
